{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "754bc5a5-84b3-4e47-8c2c-e1539fd0ed53",
   "metadata": {},
   "source": [
    "# Classifying Trees with Logistic Regression\n",
    "\n",
    "How often do you think about the trees you see every day? The \"sylvan\" part of Pennsylvania refers to trees after all, and though we're surrounded by trees in urban, suburban, and rural environments it's easy for them to become simply part of the scenery. But trees are an essential part of our environment and have been of steady interest to ecologists.\n",
    "\n",
    "Recently, a group of ecologists created a dataset of over 5 million trees from 63 US cities in order to better understand the biodiversity of urban landscapes. Here's the abstract from their study:\n",
    "\n",
    ">Sustainable cities depend on urban forests. City trees -- a pillar of urban forests -- improve our health, clean the air, store CO2, and cool local temperatures. Comparatively less is known about urban forests as ecosystems, particularly their spatial composition, nativity statuses, biodiversity, and tree health. Here, we assembled and standardized a new dataset of N=5,660,237 trees from 63 of the largest US cities. The data comes from tree inventories conducted at the level of cities and/or neighborhoods. Each data sheet includes detailed information on tree location, species, nativity status (whether a tree species is naturally occurring or introduced), health, size, whether it is in a park or urban area, and more (comprising 28 standardized columns per datasheet). This dataset could be analyzed in combination with citizen-science datasets on bird, insect, or plant biodiversity; social and demographic data; or data on the physical environment. Urban forests offer a rare opportunity to intentionally design biodiverse, heterogenous, rich ecosystems.\n",
    "\n",
    "In this week's lab, we'll use this data to classify trees with logistic regression. Can we correctly predict whether a tree is naturally occurring or was introduced into its environment?\n",
    "\n",
    "As usual, you'll follow the steps that we worked through in class.\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "- Go to the page on Data Dryad for this [urban forests data set](https://doi.org/10.5061/dryad.2jm63xsrf). Instead of having a file posted to our website, I'd like you to download the file for Pittsburgh directly from its source. *Do not simply click on the \"Download Dataset\" button!* Instead, find the Pittsburgh dataset in the dropdown menu and download just that file. **Load your data into this notebook and take a look at it. Describe the dataset and note any unusual features.**\n",
    "- This data has some issues with NA values. **Write some code to remove columns that have *only* null values in them.** This will require a little research of the `dropna()` method in the Pandas documentation.\n",
    "- In the `native` column, null values are expressed as the string \"no_info\". **Write some code to replace any instance of \"no_info\" with an NA, then drop all the NAs from this column.** Again, you made need to look at the documentation or Google a solution to this.\n",
    "- Our target variable here will be `native`, which tells us whether a tree is introduced or naturally occurring. There are lots of other variables, but only a handful will be good to use as predictors. **Create a smaller dataframe with only the following columns: `native`, `ward`, `overhead_utility`, `condition`, and `height_M`.**\n",
    "- This new dataframe contains a mixture of numerical and categorical variables. Note that `ward` is a categorical variable that's expressed as a number, and we'll need to fix that before we proceed. **Using the `astype()` method, write some code to change the `ward` column to the `category` data type. Verify that it worked with the `info()` method.**\n",
    "- **Finally, drop all null values from this new, curated dataframe.** You're ready to get started!\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's learn a little more about this data set before moving on to modeling. **Write some code and/or some markdown to answer the following questions:**\n",
    "\n",
    "- What are the different categories in the `condition` column? What does condition seem to be telling you?\n",
    "- What do you think the `overhead_utility` column is showing?\n",
    "- How do the heights of trees differ across the two `native` categories? Can you visualize this difference and talk about what you see? (Hint: a box plot would probably be best here.)\n",
    "- What will be the predictor variables in your model? Assemble these variables into a list to use on the next step.\n",
    "- You don't need to check for multicollinearity this time! Why not?\n",
    "\n",
    "## Building Your Model\n",
    "\n",
    "- Do you need to use reference coding this time? **If so, write some code to create dummy variables for your categorical predictors.**\n",
    "- **Split your data into training and test sets.** How large should your test set be?\n",
    "- **Create your model and fit it to the data.** You will need to set parameters for your model, but these will be a little different. Since we have a much larger dataset, let's use the `'sag'` solver, but keep penalty set to \"none\". You may get a warning that your model isn't \"converging,\" which means that the coefficients aren't resolving to specific values as the model iterates. To fix this, you can change the `max_iter` parameter: the default is 100 iterations, but you will probably need it much higher than that.\n",
    "- **Print the intercept and the coefficients, and interpret some of the coefficients.** You don't need to interpret *all* of the coefficients, since there will be a lot of them.\n",
    "\n",
    "## Prediction and Model Assessment\n",
    "\n",
    "- **Create a variable to store the names of the two main *categories* we are predicting.**\n",
    "- **Predict the probabilities for the test data, put them into a dataframe, and describe what you see.** Based on browsing this list, does it look like the model did a good job?\n",
    "- **Predict the categories for the test data, and store this in a variable for the next steps.**\n",
    "- **Visualize the confusion matrix for your model, and interpret the visualization.** Are there lots of false positives? false negatives? What could be going on here?\n",
    "- **Calculate scores for accuracy, precision, and recall. Explain how your model did based on these scores.**\n",
    "- **Visualize the ROC Curve, and calculate the AUC score for this curve. Explain how your model did based on these metrics.**\n",
    "- **Write a brief conclusion summarize how your model did and what you learned about the data. What would you recommend we try as a next step, to get better results?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
